# Log Read的缺点

    目前为止的实现，对于客户端发来的读请求，都是先送到raft层做共识，再拿出来执行，读取结果返回给客户端。这称为**Log Read**。实际上这是不必要的。因为每次 Read 都需要走 Raft 流程，走 Raft Log不仅仅有日志落盘的开销，还有日志复制的网络开销，另外还有一堆的 Raft “读日志” 造成的磁盘占用开销，导致 Read 操作性能是非常低效的，所以在读操作很多的场景下对性能影响很大，在读比重很大的系统中是无法被接受的，通常都不会使用。
存储日志是为了保障服务器crash后能够通过日志重放恢复到crash前的状态机，而同步日志是为了保障分布式服务器状态机之间的一致性。但是，读请求并不会改变状态机，因此没必要加重日志的存储和同步负担。另一边，为了保证线性一致性，要求读请求不能读出过期的值，因此都是路由到leader，leader下推到共识层再执行应答，这无疑加重了leader的负载。因此优化方向有两个点值得考虑：

1. 读请求不再走raft共识层，减轻共识层的日志同步开销和存储压力
2. 读请求可以在follower上面处理

庆幸的是，在保证线性一致性的情况下，raft原文提到了两种优化方案，它们均在生产环境中得到了普遍实践，例如：[https://www.sofastack.tech/blog/sofa-jraft-linear-consistent-read-implementation/](https://www.sofastack.tech/blog/sofa-jraft-linear-consistent-read-implementation/)
[https://pingcap.com/zh/blog/linearizability-and-raft#%E7%BA%BF%E6%80%A7%E4%B8%80%E8%87%B4%E6%80%A7%E5%92%8C-raft](https://pingcap.com/zh/blog/linearizability-and-raft#%E7%BA%BF%E6%80%A7%E4%B8%80%E8%87%B4%E6%80%A7%E5%92%8C-raft)
(https://tanxinyu.work/consistency-and-consensus/#etcd-%E7%9A%84-Raft)

### ReadIndex Read

    第一种是 ReadIndex Read，当 Leader 需要处理 Read 请求时，Leader 与过半机器交换心跳信息确定自己仍然是 Leader 后可提供线性一致读：

1. 如果 Leader 在当前任期还没有提交过日志，先提交一条空日志（如果raft层实现了leader上任后就append空日志的机制，则不需要考虑此条）
2. Leader 将自己当前 Log 的 commitIndex 记录到一个 Local 变量 ReadIndex 里面；
3. 接着向 Followers 节点发起一轮 Heartbeat，如果半数以上节点返回对应的 Heartbeat Response，那么 Leader就能够确定现在自己仍然是 Leader；
4. Leader 等待自己的 StateMachine 状态机执行，至少应用到 ReadIndex 记录的 Log，直到 applyIndex 超过 ReadIndex，这样就能够安全提供 Linearizable Read，也不必管读的时刻是否 Leader 已飘走；
5. Leader 执行 Read 请求，将结果返回给 Client。

    第1条是为了保证leader能够确定2中真正的commitIndex。因为新选举产生的 Leader，它虽然有全部 committed Log，但它并不能直接commit过往任期的log，也即无法确定集群中最新的commitIndex，状态机可能落后于之前的 Leader，只能通过commit当前 term 的 Log后才能确定最新commitIndex，从而保证新 Leader 的状态机一定新于旧 Leader，之后肯定不会出现 stale read。这一点在做lab2时，raft原文Figure8就指出了这一情况。第3点是为了保证该leader不是少数区的leader。
使用 ReadIndex Read 提供 Follower Read 的功能，很容易在 Followers 节点上面提供线性一致读，Follower 收到 Read 请求之后：

1. Follower 节点向 Leader 请求最新的 ReadIndex；
2. Leader 仍然走一遍之前的流程，执行上面前 3 步的过程(确定自己真的是 Leader)，并且返回 ReadIndex 给 Follower；
3. Follower 等待当前的状态机的 applyIndex 超过 ReadIndex；
4. Follower 执行 Read 请求，将结果返回给 Client。

    不同于通过 Raft Log 的 Read，ReadIndex Read 使用 Heartbeat 方式来让 Leader 确认自己是 Leader，省去 Raft Log 流程。相比较于走 Raft Log 方式，ReadIndex Read 省去磁盘的开销，能够大幅度提升吞吐量。虽然仍然会有网络开销，但是 Heartbeat 本来就很小，所以性能还是非常好的。


## Beware-文档实现思路

### Readindex on leader：

- 空日志三种解决方案：
  
  - 应用层开启后台协程，定时询问raft层是否有当前任期的日志，没有则push一条空日志到下层
  - 在应用层的readindex处理函数中，确认下层是否commit了当前任期的日志，如果没有，push一条空日志
  - raft层，leader一上任就append一条空日志（不兼容lab2的测试）

- 利用struct{}类型表示空日志，相比单独定义一个空命令类型更省空间，同时为了能在labgob中编码解码传输，需要在所有使用了raft层的上层启动函数中中注册struct{}：labgob.Register(struct{}{})
  
  - golang 空结构体的使用 [https://geektutu.com/post/hpg-empty-struct.html](https://geektutu.com/post/hpg-empty-struct.html)
  - 空结构体解析[https://zhuanlan.zhihu.com/p/351176221](https://zhuanlan.zhihu.com/p/351176221)
  - 建议配合空接口 interface{}一起食用

- 不能使用nil表示空日志[https://zhuanlan.zhihu.com/p/295048056](https://zhuanlan.zhihu.com/p/295048056)

- 同时在shardctrler中也要注册struct{}类型，并且在其apply loop中添加对struct{}的处理分支，因为leader上任append空日志是raft层的行为，上层应用不管是shardctrler还是shardkv都可能收到struct{}{}。

- applyloop中，每更新一次lastapplied，并且该命令应用执行完到状态机，就broadcast一次
  
  - 修改了检测lastapplied代码的位置，应该放在最前

- 不要在锁内使用channel，可能阻塞，最好unlock再lock，或者对使用channel的语句使用协程走掉。

- 更改handle函数的嵌套层级逻辑，修改为线性逻辑，提高可读性

- leader用心跳确认leadership，等待心跳回复时，设置超时机制（实测，可能会更慢）

- defer 执行顺序是栈序，按defer语句的定义位置倒序执行
  
  ### Follower Read ：

- 如果实现了Follower Read，那么client在发出GET请求时，不用再缓存并使用上一轮请求的leader号/follower号，而是应该随机请求一个服务器即可，否则的话大部分GET请求还是路由到了leader身上，没有达到平均负载的作用。
  
  - 或者划分客户端分组，每组固定分配一个follower，优先访问它，访问失败再换服务器，这样请求的去重回复也能够最高效发挥作用，否则随机访问服务器，在去重时很难命中

- 注意，引入Follower Read之后，第3点不能只在状态机apply log的时候通知，而是状态机lastapplied一更新发生变化就要通知。因为follower不同于leader，它可能收到来自leader的snapshot而不是log同步，然后在状态机安装该snapshot，如果不在这里通知，那么恰好在安装snapshot之后没有其他log同步，客户端将永远wait阻塞，即使follower的状态机已经通过安装snapshot达到了readindex apply后的状态。

- follower询问leader要readindex，不能复用应答客户端的GET handle RPC函数，否则可能会引起链式递归调用。应该把寻找leader的逻辑循环限制在第一个follower身上，这样更简单简洁。

- follower需要知道当前的leader号，为了避免每次都随机找leader，因此在raft结构体中新增一个非持久化状态leaderID：
  
  - 服务器初始状态、重启、转为PreCandidate、Candidate均把leaderID设为-1
  - follower在appendentriesRPC中收到来自当前**合法**leader的请求，则更新leader号。
  - leader一上任把rf.leaderID更新为自己（虽然没太必要，一般是通过GetState()判断leader身份）
  - 注意follower不能在RequestVote中，投出票后更新leaderID，因为即使投票，Candidate也有可能选举失败。只有在成功接收并处理appendentries时，才更新leaderID。

- safeReadindex的整个逻辑到底是放在raft层还是应用层。
  
  - 实现在raft层的优点，应用层收到Get请求，需要先调用raft层的方法获取下层raft的状态信息（如自己身份，leaderID），再根据该状态去处理对应分支。我在想这样是不是不够优雅，应用层和raft层耦合在了一起，因为应用层其实没必要去专门关心raft层到底是leader还是follower还是candidate，当前的leaderID是谁，怎么处理路由到leader等等。可以把判断身份、leader直接readindex、或follower路由到leader要readindex的过程直接抽象成raft层的接口，对应用层透明，整个过程在raft层执行，最后只返回给应用层readindex和Err即可。
  - 把调用逻辑实现在应用层的优点。因为shard配置和变更、执行结果的缓存只有应用层能感知到。考虑这样一种情况：假设leader收到了重新配置信息，失去了shard1的所有权，随后将该配置下推到raft层并成功复制到了多数服务器，那么leader这侧会领先follower将新配置apply到状态机。此时，可能某个follower还未来得及向其应用层apply该配置，然后该follower收到了客户端的关于shard1的Get请求。**也即Get发生在新配置之后。**如果safeReadindex过程全部实现在raft层，它会先在应用层检测shard1所有权，发现合法，则启动raft层的safeReadindex，路由到leader要readindex，但raft层的leader并不能感知到上层已经失去了shard1，会正常返回readindex。此后，follower从raft层拿到readindex，将在应用层等待 readindex 被apply，显然当它等待完成，执行读状态机的时候，才会发现shard不合法（此时新配置肯定已经apply了），直到这里才能回复客户端ErrWrongGroup。
  - 如果调用和路由逻辑实现在应用层，follower照常会在应用层检测shard合法性，之后询问raft层自己身份并索要leader号，再在应用层路由到leader，向leader发起应用层的读readindex RPC，leader收到后在先应用层检测shard合法，合法则调用raft层读readindex，不合法即可直接返回ErrWrongGroup。

- 让我们再来考虑safeReadindex全部实现在raft层，当**follower在新配置到达leader之前收到Get请求**的情况。
  
  - 一种正常的情况是，follower先收到Get请求，在raft层路由到leader要到readindex之后，leader收到了配置信息，失去了shard1的所有权，下推到raft层做同步。此时follower拿到的readindex一定是先于新配置被apply，那么follower会读状态机中的shard1，并向客户端回复Get请求。这一过程并没有违反线性一致读。
  - 另一种情况，follower先收到Get请求，在raft层路由到leader要到readindex之前，leader收到了配置信息，失去了shard1的所有权，下推到raft层做同步，并完成同步到多数服务器。此时follower拿到的readindex是等于或迟于新配置被apply的，那么当follower读状态机中的shard1，会向客户端回复ErrWrongGroup。这一过程仍然并没有违反线性一致读。

- 总结：也即引入follower的辅助读之后，**线性一致性读中的Get请求发生的时间点，不是follower收到Get的时间点，而应该是leader拿到readindex的时间点。因为leader的状态机和raft log永远是集群中最新的，强leader特性要求raft的数据必须从leader单向流向foloower，一切以leader为准，避免了follower可能绕开leader刚刚收到的最新状态，从而提供违反线性一致读的服务。**

- 这种 Follower Read 的实现方式仍然会有一次到 Leader 请求 Commit Index 的 RPC，所以目前的 Follower read 实现在降低延迟上不会有太多的效果。虽然对于延迟来说，不会有太多的提升，但是对于提升读的吞吐，减轻 Leader 的负担还是很有帮助的。总体来说是一个很好的优化。
  
  ### Lease Read
  
  ReadIndex虽然提升了只读请求的吞吐量，但是由于其还需要一轮心跳广播，还是存在 Heartbeat 网络开销，因此只读请求延迟的优化并不明显。而Lease Read在损失了一定的安全性的前提下，进一步地优化了延迟。
  Raft 论文里面提及一种通过 Clock + Heartbeat 的 Lease Read 优化方法，也就是 Leader 发送 Heartbeat 的时候首先记录一个时间点 Start，当系统大部分节点都回复 Heartbeat Response，由于 Raft 的选举机制，Follower 会在 Election Timeout 的时间之后才重新发生选举，下一个 Leader 选举出来的时间保证大于 Start+Election Timeout/Clock Drift Bound，所以可以认为 Leader 的 Lease 有效期可以到 Start+Election Timeout/Clock Drift Bound 时间点。Lease Read 与 ReadIndex 类似但更进一步优化，不仅节省 Log，而且省掉网络交互，大幅提升读的吞吐量并且能够显著降低延时。
  Lease Read 基本思路是 Leader 取一个比 Election Timeout 小的租期（最好小一个数量级），在租约期内不会发生选举，确保 Leader 不会变化，所以跳过 ReadIndex 的第二步也就降低延时。由此可见 Lease Read 的正确性和时间是挂钩的，依赖本地时钟的准确性，因此虽然采用 Lease Read 做法非常高效，但是仍然面临风险问题，也就是存在预设的前提即各个服务器的 CPU Clock 的时间是准的，即使有误差，也会在一个非常小的 Bound 范围里面，时间的实现至关重要，如果时钟漂移严重，各个服务器之间 Clock 走的频率不一样，这套 Lease 机制可能出问题。
  Lease Read 实现方式包括：
1. 定时 Heartbeat 获得多数派响应，确认 Leader 的有效性；
2. 在租约有效时间内，可以认为当前 Leader 是 Raft Group 内的唯一有效 Leader，可忽略 ReadIndex 中的 Heartbeat 确认步骤(2)；
3. Leader 等待自己的状态机执行，直到 applyIndex 超过 ReadIndex，这样就能够安全的提供 Linearizable Read。

尽量避免时钟漂移的影响：

1. 缩小leader持有lease的时间，选择了一个略小于election timeout的时间，以减小时钟漂移带来的影响
   
   ## **debug？**
- 8月11晚
- 原本是matchindex，和不min
  - 单独修改无用：已修改raft_appendentries 129行  ， max（matchindex[server] ** +1，**
  - 已修改raft_appendentries 129行  ， max（nextindex[server] ** ，**
  - **已修改155行改回min**
